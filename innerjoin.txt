[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202307190847_1093430308.txt
hive> select * from customers INNER JOIN purchase ON customers.userid =purchase.userid;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_2023071905220004, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job202307190522_0004
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307190522_0004
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-19 08:56:22,501 Stage-1 map = 0%,  reduce = 0%
2023-07-19 08:56:28,594 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.39 sec
2023-07-19 08:56:29,607 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.39 sec
2023-07-19 08:56:30,618 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.39 sec
2023-07-19 08:56:31,644 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.39 sec
2023-07-19 08:56:32,671 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.39 sec
2023-07-19 08:56:33,699 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.41 sec
2023-07-19 08:56:34,712 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.41 sec
2023-07-19 08:56:35,727 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.41 sec
2023-07-19 08:56:36,738 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.41 sec
MapReduce Total cumulative CPU time: 5 seconds 410 msec
Ended Job = job_202307190522_0004
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 5.41 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 410 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:05:00.0	100
2	Jane Smith	jane.smith@example.com	2	2023-01-01 10:08:00.0	150
3	Robert Johnson	robert.johnson@example.com	3	2023-01-01 10:09:00.0	200
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:13:00.0	120
5	Michael Wilson	michael.wilson@example.com	5	2023-01-01 10:17:00.0	80
Time taken: 21.413 seconds
hive> select * from custemers INNER JOIN clickstream ON customers.userid =clickstream.userid;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_2023071905220005, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job202307190522_0005
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307190522_0005
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-19 08:58:52,508 Stage-1 map = 0%,  reduce = 0%
2023-07-19 08:58:57,554 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.35 sec
2023-07-19 08:58:58,563 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.35 sec
2023-07-19 08:58:59,579 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.35 sec
2023-07-19 08:59:00,589 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.35 sec
2023-07-19 08:59:01,614 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.35 sec
2023-07-19 08:59:02,628 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.3 sec
2023-07-19 08:59:03,655 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.3 sec
2023-07-19 08:59:04,671 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.3 sec
2023-07-19 08:59:05,698 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.3 sec
MapReduce Total cumulative CPU time: 4 seconds 300 msec
Ended Job = job_202307190522_0005
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 4.3 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 300 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:01:00.0	product_page
1	John Doe	john.doe@example.com	1	2023-01-01 10:00:00.0	homepage
2	Jane Smith	jane.smith@example.com	2	2023-01-01 10:02:00.0	homepage
2	Jane Smith	jane.smith@example.com	2	2023-01-01 10:03:00.0	cart_page
3	Robert Johnson	robert.johnson@example.com	3	2023-01-01 10:07:00.0	cart_page
3	Robert Johnson	robert.johnson@example.com	3	2023-01-01 10:05:00.0	homepage
3	Robert Johnson	robert.johnson@example.com	3	2023-01-01 10:06:00.0	product_page
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:09:00.0	homepage
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:10:00.0	product_page
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:11:00.0	cart_page
4	Lisa Brown	lisa.brown@example.com	4	2023-01-01 10:12:00.0	checkout_page
5	Michael Wilson	michael.wilson@example.com	5	2023-01-01 10:15:00.0	homepage
5	Michael Wilson	michael.wilson@example.com	5	2023-01-01 10:16:00.0	product_page
Time taken: 18.535 seconds
hive> select * from purchase INNER JOIN clickstream ON purchase.userid =clickstream.userid;  
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307190522_0006, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307190522_0006
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307190522_0006
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-19 09:00:54,920 Stage-1 map = 0%,  reduce = 0%
2023-07-19 09:00:59,945 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.16 sec
2023-07-19 09:01:00,955 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.37 sec
2023-07-19 09:01:01,968 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.37 sec
2023-07-19 09:01:02,983 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.37 sec
2023-07-19 09:01:04,001 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.37 sec
2023-07-19 09:01:05,010 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.35 sec
2023-07-19 09:01:06,019 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.35 sec
2023-07-19 09:01:07,028 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.35 sec
2023-07-19 09:01:08,234 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.35 sec
MapReduce Total cumulative CPU time: 4 seconds 350 msec
Ended Job = job_202307190522_0006
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 4.35 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 350 msec
OK
1	2023-01-01 10:05:00.0	100	1	2023-01-01 10:01:00.0	product_page
1	2023-01-01 10:05:00.0	100	1	2023-01-01 10:00:00.0	homepage
2	2023-01-01 10:08:00.0	150	2	2023-01-01 10:02:00.0	homepage
2	2023-01-01 10:08:00.0	150	2	2023-01-01 10:03:00.0	cart_page
3	2023-01-01 10:09:00.0	200	3	2023-01-01 10:07:00.0	cart_page
3	2023-01-01 10:09:00.0	200	3	2023-01-01 10:05:00.0	homepage
3	2023-01-01 10:09:00.0	200	3	2023-01-01 10:06:00.0	product_page
4	2023-01-01 10:13:00.0	120	4	2023-01-01 10:09:00.0	homepage
4	2023-01-01 10:13:00.0	120	4	2023-01-01 10:10:00.0	product_page
4	2023-01-01 10:13:00.0	120	4	2023-01-01 10:11:00.0	cart_page
4	2023-01-01 10:13:00.0	120	4	2023-01-01 10:12:00.0	checkout_page
5	2023-01-01 10:17:00.0	80	5	2023-01-01 10:15:00.0	homepage
5	2023-01-01 10:17:00.0	80	5	2023-01-01 10:16:00.0	product_page
Time taken: 18.235 seconds
hive>